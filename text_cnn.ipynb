{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cf53ab-d611-402a-8f04-058103abf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops\n",
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1e8748-e92f-4c4a-bb7f-9aaaa3b1d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': 'polarity/',\n",
    "    'device_target': 'CPU',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c404eac-df85-477d-9eb1-cec52be4f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4dca0a-48ab-48a3-a77d-b76bbeea8842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"polarity/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "    print(\"Negative reivews:\")\n",
    "    for i in range(5):\n",
    "        print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"polarity/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "    print(\"Positive reivews:\")\n",
    "    for i in range(5):\n",
    "        print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b31da28-cd54-487f-b7e0-8b60b653cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return (np.array(self.input_list[item][0], dtype=np.int32),\n",
    "               np.array(self.input_list[item][1], dtype=np.int32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a583066-6f72-46b4-bd48-cead256e75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReview():\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {'neg':0, 'pos':1}\n",
    "        self.files = []\n",
    "        self.doCovert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"check the root_dir\")\n",
    "            raise ValueError\n",
    "            \n",
    "        for root, _, filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root, each))\n",
    "            break\n",
    "            \n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "            \n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float('inf')\n",
    "        self.maxlen = float('-inf')\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        \n",
    "        for filename in self.files:\n",
    "            self.read_data(filename)\n",
    "            \n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "        \n",
    "    def read_data(self, filePath):\n",
    "        with open(filePath, 'r') as f:\n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence, self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence, self.feelMap['neg']])\n",
    "       \n",
    "    # Transform text in vector\n",
    "    def text2vec(self, maxlen):\n",
    "        self.Vocab = dict()\n",
    "        \n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "            self.doCovert = True\n",
    "                    \n",
    "    # Divide dataset in train and test\n",
    "    def split_dataset(self, split):\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        \n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "        \n",
    "        random.shuffle(self.train)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        if self.doCovert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "        \n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.train),\n",
    "            column_names=['data', 'label'],\n",
    "            shuffle=False)\n",
    "        dataset=dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.test),\n",
    "            column_names=['data', 'label'],\n",
    "            shuffle=False)\n",
    "        dataset=dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "448850ee-5deb-40e4-9551-d911e2f6f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size, epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c873920-4801-4914-8215-6ed4fa0ded08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[    0,   273,  5325 ...     0,     0,     0],\n",
      " [    0,   273,   175 ...     0,     0,     0],\n",
      " [ 2450,  5103,   195 ...     0,     0,     0],\n",
      " ...\n",
      " [ 2999,    91,     0 ...     0,     0,     0],\n",
      " [   62,   416, 17214 ...     0,     0,     0],\n",
      " [   15,   146,   110 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, \n",
      " 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, \n",
      " 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])}\n",
      "[   0  273  175 1081  725   10 8335  904    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item = dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i < 1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f80cb27-5f45-4964-b907-b06982d0b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "                                     \n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "                  \n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in range(cfg.batch_size - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b63efb-2370-47e5-8b9a-666f2aa763b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                    pad_mode = 'pad', weight_init=weight, has_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f156bfe2-ba3e-4939-b673-688cae112724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "        \n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "        \n",
    "        self.concat = ops.Concat(1)\n",
    "        \n",
    "        self.fc = nn.Dense(96 * 3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "        [\n",
    "            make_conv_layer((kernel_height, self.vec_length)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(self.word_len - kernel_height + 1, 1))\n",
    "        ])\n",
    "    \n",
    "    def construct(self, x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "        \n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "        \n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len,\n",
    "             num_classes=cfg.num_classes, vec_length=cfg.vec_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b63e763-b672-4fcf-a9b3-be76e9f62cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, loss function, checkpoint, and time monitor settings\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()),\n",
    "    learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "\n",
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})\n",
    "\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2),\n",
    "    keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6793bd8f-aec3-413c-911f-00c419aec06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "# print('train success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3166825-3112-489d-8e2b-55ade9d49bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                        .replace('\"','')\\\n",
    "                        .replace('\\'','')\\\n",
    "                        .replace('.','')\\\n",
    "                        .replace(',','')\\\n",
    "                        .replace('[','')\\\n",
    "                        .replace(']','')\\\n",
    "                        .replace('(','')\\\n",
    "                        .replace(')','')\\\n",
    "                        .replace(':','')\\\n",
    "                        .replace('--','')\\\n",
    "                        .replace('-',' ')\\\n",
    "                        .replace('\\\\','')\\\n",
    "                        .replace('0','')\\\n",
    "                        .replace('1','')\\\n",
    "                        .replace('2','')\\\n",
    "                        .replace('3','')\\\n",
    "                        .replace('4','')\\\n",
    "                        .replace('5','')\\\n",
    "                        .replace('6','')\\\n",
    "                        .replace('7','')\\\n",
    "                        .replace('8','')\\\n",
    "                        .replace('9','')\\\n",
    "                        .replace('`','')\\\n",
    "                        .replace('=','')\\\n",
    "                        .replace('$','')\\\n",
    "                        .replace('/','')\\\n",
    "                        .replace('*','')\\\n",
    "                        .replace(';','')\\\n",
    "                        .replace('<b>','')\\\n",
    "                        .replace('%','')\\\n",
    "                        .replace(\" \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    \n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word + \" does not appear in the dictionary.\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    \n",
    "    sentence = vector\n",
    "    return sentence\n",
    "\n",
    "# Use this function to make inference about a new review\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print('Positive comments')\n",
    "    else:\n",
    "        print('Negative comments') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb03e642-6ccf-4889-8f93-ab332d7aebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments\n"
     ]
    }
   ],
   "source": [
    "example_a = 'it was so boring'\n",
    "example_b = 'one of the best'\n",
    "example_c = 'it could be better'\n",
    "\n",
    "# You can create you own example and pass as a argument\n",
    "inference(example_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
